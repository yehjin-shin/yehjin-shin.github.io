---
---

@inproceedings{shin2023attentive,
  title={An Attentive Inductive Bias for Sequential Recommendation Beyond the Self-Attention},
  author={Shin, Yehjin and Choi, Jeongwhan and Wi, Hyowon and Park, Noseong},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  arxiv={2312.10325},
  abstract={Sequential recommendation (SR) models based on Transformers have achieved remarkable successes. The self-attention mechanism of Transformers for computer vision and natural language processing suffers from the oversmoothing problem, i.e., hidden representations becoming similar to tokens. In the SR domain, we, for the first time, show that the same problem occurs. We present pioneering investigations that reveal the low-pass filtering nature of self-attention in the SR, which causes oversmoothing. To this end, we propose a novel method called Beyond Self-Attention for Sequential Recommendation (BSARec), which leverages the Fourier transform to i) inject an inductive bias by considering fine-grained sequential patterns and ii) integrate low and high-frequency information to mitigate oversmoothing. Our discovery shows significant advancements in the SR domain and is expected to bridge the gap for existing Transformer-based SR models. We test our proposed approach through extensive experiments on 6 benchmark datasets. The experimental results demonstrate that our model outperforms 7 baseline methods in terms of recommendation performance.},
  year={2024},
  pdf={https://arxiv.org/pdf/2312.10325.pdf},
  code={https://github.com/yehjin-shin/BSARec},
  selected=true
}

@inproceedings{wi2023continuous,
  title={Continuous-time Autoencoders for Regular and Irregular Time Series Imputation},
  author={Wi, Hyowon and Shin, Yehjin and Park, Noseong and Woo, Sungpil and Lim, Sunhwan},
  booktitle={Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
  abstract={Time series imputation is one of the most fundamental tasks for time series. Real-world time series datasets are frequently incomplete (or irregular with missing observations), in which case imputation is strongly required. Many different time series imputation methods have been proposed. Recent self-attention-based methods show the state-of-the-art imputation performance. However, it has been overlooked for a long time to design an imputation method based on continuous-time recurrent neural networks (RNNs), i.e., neural controlled differential equations (NCDEs). To this end, we redesign time series (variational) autoencoders based on NCDEs. Our method, called continuous-time autoencoder (CTA), encodes an input time series sample into a continuous hidden path (rather than a hidden vector) and decodes it to reconstruct and impute the input. In our experiments with 4 datasets and 19 baselines, our method shows the best imputation performance in almost all cases.},
  arxiv={2312.16581},
  year={2024},
  pdf={https://arxiv.org/pdf/2312.16581.pdf},
  selected=true
}


@inproceedings{kim2022sos,
  title={SOS: Score-based oversampling for tabular data},
  author={Kim, Jayoung and Lee, Chaejeong and Shin, Yehjin and Park, Sewon and Kim, Minjung and Park, Noseong and Cho, Jihoon},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
  pages={762--772},
  arxiv={2206.08555},
  abstract={Score-based generative models (SGMs) are a recent breakthrough in generating fake images. SGMs are known to surpass other generative models, e.g., generative adversarial networks (GANs) and variational autoencoders (VAEs). Being inspired by their big success, in this work, we fully customize them for generating fake tabular data. In particular, we are interested in oversampling minor classes since imbalanced classes frequently lead to sub-optimal training outcomes. To our knowledge, we are the first presenting a score-based tabular data oversampling method. Firstly, we re-design our own score network since we have to process tabular data. Secondly, we propose two options for our generation method: the former is equivalent to a style transfer for tabular data and the latter uses the standard generative policy of SGMs. Lastly, we define a fine-tuning method, which further enhances the oversampling quality. In our experiments with 6 datasets and 10 baselines, our method outperforms other oversampling methods in all cases.},
  year={2022},
  pdf={https://dl.acm.org/doi/pdf/10.1145/3534678.3539454},
  code={https://github.com/JayoungKim408/SOS},
  selected=true
}
